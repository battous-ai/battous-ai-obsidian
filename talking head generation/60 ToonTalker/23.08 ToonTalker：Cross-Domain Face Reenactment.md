# Keyword
cross-domain
# Overview

| key            | value                                                                              |
| ------------ | ------------------------------------------------------------------------------ |
| github       | https://github.com/OpenTalker/ToonTalker/ |
| paper        | ToonTalkerï¼šCross-Domain Face Reenactment |
# Architecture

![[image-20240104180039600.png]]
# Notes
- - cross-domain reenactment has many issues
	- the distribution shift between different domains
		- appearance and motion gap
	- to solve the domain shift problem, we propose a transformer-based framework to align the motions from two domains in a shared canonical latent space
- motion transfer
	- shared canonical motion space (to align motions from different domains)
	- leverage a virtual reference to model transformation (like FOMM)
- problems
	- lack of paired training data
		- analogy constraint
	- imbalanced training data (real videos >> cartoon videos)
- transformer based
- a for appearance and m for motion
- real and cartoon each has (a.k.a. domain-specific) 
	- an appearance encoder
	- a motion encoder
	- a backward transformer
	- a generator
	- a motion code base
- share
	- source query transformer
	- driving query transformer
- motion code base
	- output
		- learnable embeddings (==how to learn? never change during infer?==)
- query transformer
	- query: motion base
	- key/value: $F_s$ or $F_D$
- backward transformer
	- 3 blocks
	- query: output of the previous module
	- key/value: multi-scale features from appearance encoder
- Image generator
	- ==StyleConv block?==

![[image-20240104193529639.png]]

