- rooted in the methods of 
	- variational bayesian
	- graphical model
- map input to a distribution instead of a fixed vector
	- the encoder output of a autoencoder is just a tensor
	- the encoder output of a VAE is a tensor composed by a random variable
- we need a generative model to sample $X$ â†’
	- we need the **joint probability distribution** of $X$ and $Z$, $P(X,Z)$ ==why?==
		- parameterized by $\theta$ 
			- this is what we want to get
- $P(X,Z)=P(X)P(Z|X)=P(Z)P(X|Z)$
	- $P(Z|X)=\frac{P(Z)P(X|Z)}{P(X)}$
		- $P(Z)$ _the [[Prior Probability Distribution]] of Z_
		- $P(Z|X)$ _the posterior probability distribution of Z_
		- $P(X)$ _evidance_
		- $P(X|Z)$ the likelihood
	- for a certain value $x$ of $X$
		- $p(x)=\int_{z}p(x|z)\,dz$ 
-  the optimal parameter $\theta^*$ is the one that maximizes the probability of generating all real data samples of the dataset
	- according to the maximum likelihood estimation
	- $\theta^* = argmax_{\theta}\prod_{i=1}^n{p_\theta(x^{(i)})}$
		- $\theta^*=argmax_\theta \sum_{i=1}^nlogp_\theta(x^{(i)}))$ _convert to log probabilities_
			- with $p_\theta(x^{(i)})=\int p_\theta(x^{(i)}|z)p_\theta(z)dz$
				- $\theta^*=argmax_\theta \sum_{i=1}^nlog\int_z p_\theta(x^{(i)}|z)p_\theta(z)dz$ _convert to log probabilities_
				 - with the integral and the latent variable $z$, it is not possible to compute the parameter
					 - **introduce a new approximation function** $q_\phi(z|x)$ , parameterized by $\phi$
- ![[image-20240213204724413.png|800]]
	- the structure looks like autoencoder
		- $p_\theta(x|z)$ defines a generative model
			- similar to the decoder part of the autoencoder
			- _probabilistic decoder_
		- $q_\phi(z|x)$ 
			- similar to the encoder part of the autoencoder
			- _probabilistic encoder_
- Evidence Lower Bound (ELBO) 
