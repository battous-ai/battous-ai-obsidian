# Understanding Checklist
- [ ] the architecture of VAE network
- [ ] why ELBO
- [ ] what does the formula deduction of ELBO try to prove
- [ ] vs AE
- [ ] is it a generative model


- rooted in the methods of 
	- variational bayesian
	- graphical model
- map input to a distribution instead of a fixed vector
	- the encoder output of a autoencoder is just a tensor
	- the encoder output of a VAE is a tensor composed by a random variable
- we need a generative model to sample $X$ →
	- we need the **joint probability distribution** of $X$ and $Z$, $P(X,Z)$ ==why?==
		- parameterized by $\theta$ 
			- this is what we want to get
			
- $P(X,Z)=P(X)P(Z|X)=P(Z)P(X|Z)$
	- $P(Z|X)=\frac{P(Z)P(X|Z)}{P(X)}$
		- $P(Z)$   _// the [[Prior Probability Distribution]] of Z_
		- $P(Z|X)$   _// the posterior probability distribution of Z_
		- $P(X)$   _// the evidance_
		- $P(X|Z)$   _// the likelihood_
	- for a certain value $x$ of $X$
		- $p(x)=\int_{z}p(x,z)\,dz$ 

# How to Generate a Sample (if we know the real distribution param $\theta^*$)
1. sample a $z^{(i)}$ from a prior distribution $p_{\theta*}(z)$
2. generate a sample $x^{(i)}$ that looks like a real data point by a conditional distribution $p_\theta*(x|z=z^{(i)})$
# Intractable Maximum Likelihood Estimation
-  the optimal parameter $\theta^*$ is the one that maximizes the probability of generating all real data samples of the dataset
	- according to the maximum likelihood estimation
	- $\theta^* = argmax_{\theta}\prod_{i=1}^n{p_\theta(x^{(i)})}$
		- $\theta^*=argmax_\theta \sum_{i=1}^nlogp_\theta(x^{(i)}))$    _// convert to log probabilities_
			- with $p_\theta(x^{(i)})=\int p_\theta(x^{(i)},z)p_\theta(z)dz$
				- $\theta^*=argmax_\theta \sum_{i=1}^nlog\int_z p_\theta(x^{(i)},z)p_\theta(z)dz$
				 - with the integral and the latent variable $z$, it is not possible to compute the parameter
					 - **introduce a new approximation function** $q_\phi(z|x)$ , parameterized by $\phi$
# the Approximation (ELBO)
- Evidence Lower Bound (ELBO) 
	- $\theta^*=argmax_\theta \sum_{i=1}^nlogp_\theta(x^{(i)}))$
		- $log\,p_\theta(x)$   _// ignore the sum part_
		     $=log\,\int_z\,p_\theta(x,z)$
			 $=log \int_z q_\phi(z)\frac{p_\theta(x,z)}{q_\phi(z)}$
			 $=log E_{q_\phi} (z)[\frac{p_\theta(x,z)}{q_\phi(z)}]$
			 $\geq E_{q_\phi}(z)log[\frac{p_\theta(x,z)}{q_\phi(z)}]$    _// according to the [[Jensen's inequality]]_
			 $=\int_z q_\phi(z) log[\frac{p_\phi(x,z)}{q_\phi(z)}]$
			 $=[\int_zq_\phi(z)log\,p_\theta(x,z)]-\int_zq_\phi(z)log\,q_\phi(z)]$
			 $=E_{q_\phi}(z)[log\,p_\theta(x,z)]-E_{q_\phi}(z)[log\,q_\phi(z)]$
- because $p(x,z)=p(x)p(z|x)$ 
	- $E_{q_\phi}(z)[log\,p_\theta(x,z)]-E_{q_\phi}(z)[log\,q_\phi(z)]$    
		$=E_{q_\phi}(z)[log\,p_\theta(x)+log\,p_\theta(z|x)]-E_{q_\phi}(z)[log\,q_\phi(z)]$
		$=\underbrace{E_{q_\phi}(z)[log\,p_\theta(x)]}_{\text{has nothing to do with z, the E can be removed}}+E_{q_\phi}(z)[log\,p_\theta(z|x)]-E_{q_\phi}(z)[log\,q_\phi(z)]$ 
		$=\underbrace{log\,p_\theta(x)}_{\text{MLE}}+\underbrace{E_{q_\phi}(z)[log\,p_\theta(z|x)]-E_{q_\phi}(z)[log\,q_\phi(z)]}_{\text{KL divergence}}$ 
		$=\underbrace{log\,p_\theta(x)}_{\text{MLE}}+\underbrace{KL({q_\phi}(z)||log\,p_\theta(z|x)])}_{\text{KL divergence}}$ 
	- if $q_\phi(z)=p_\theta(z|x)$, then $KL({q_\phi}(z)||log\,p_\theta(z|x)]=0$
		- maximize ELBO equals maximize MLE

- $E_{q_\phi}(z)[log\,p_\theta(x,z)]-E_{q_\phi}(z)[log\,q_\phi(z)]$    because $P(x,z)=P(x)P(z|x)=P(z)P(x|z)$

# the Architecture (looks like an autoencoder)
- ![[image-20240213204724413.png|800]]
		- $p_\theta(x|z)$ defines a generative model
				- similar to the decoder part of the autoencoder
			- _probabilistic decoder_
		- $q_\phi(z|x)$ 
			- similar to the encoder part of the autoencoder
			- _probabilistic encoder_

# the Architecture (again)
- ![[image-20240214113900871.png]]
- ![[image-20240215190513693.png]]
- $Z$ is a random variable with multivariate normal distribution
	- $P(Z) \sim \mathcal{N}(0, I)$   _// we choose it to be this way_
- the encoder part
	- the posterior probability distribution of $Z$ is also a gaussian distribution
		- $q_\phi(z|x)= \mathcal{N}(\mu_z, \sigma_z)$
			- $\sigma_z$ is a diagnal matrix
			- **act as the encoder part**
				- $\phi$ : the encoder network params
				- encoder outputs
					- $\mu_z = encoder_\phi(x)$
					- $\sigma_z=encoder_\phi(x)$
	- reparameterization trick
- the decoder part
	- $p_\theta(x|z)$  _//  the reconstructed $X$ is also considered as Gaussian_
		- $\theta$ : the decoder network params
		- decoder outputs
			- $\mu_x=decoder_\theta(z)$
			- $\sigma_x$ is assumed to be $I$

为什么比ae好
decoder怎么工作，怎么sample mapping?

intractable → ELBO → Markvo Chain Monte Carlo → reparameterization trick → the analystic form of encoder and decoder and ELBO → training loss 


# Reference
- https://lilianweng.github.io/posts/2018-08-12-vae/
- https://www.zhangzhenhu.com/aigc/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8.html
___
**probabilistic nature → generative model**
___
